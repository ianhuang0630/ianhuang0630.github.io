<!DOCTYPE html>
<html lang="en">

<head>
    <title> Publications </title>
    <link href="https://fonts.googleapis.com/css?family=Montserrat" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css?family=Libre+Barcode+39+Extended" rel="stylesheet">
    <link rel="stylesheet" type= "text/css" href="styles/style.css">
    <link rel="stylesheet" type= "text/css" href="styles/style_publications.css">
</head>
<body>
    <div class="tabs">
        <a href="index.html" class="tab0">Me</a> |
        <a href="publications.html" class="tab1">Publications</a> | 
        <a href="pet_projects.html" class="tab2">Pet Projects</a> |
        <a href=assets/IanHuangCV.pdf class="tab3">Resume/CV</a> |
        <a href="https://github.com/ianhuang0630" class="tab4">GitHub</a> |
        <a href="contact.html" class="tab5">Contact</a>
    </div>
   
    <div class="centerhead">
        <div class="headers">
            <h1 id="header1">Publications</h1>
            <h2 id="header2">(and works in progress)</h2>
        </div>
    </div> 
    
    <div class="researchpaperlist fade-in">
        <!-- list of publications -->
        <div> 

    <div class="researchpaper">
	    <div class="paper-image">
		    <img src="assets/pictures/paper7.png" class="paper_pic">
	    </div>
	    <div class="paper-description">
		<h3 class=tab4> </h3>
		<i> Submitted to EMNLP2022. </i>
		<p>
       	Natural language interaction is a promising direction for 
		democratizing 3D shape design. However, existing methods 
		for text-driven 3D shape editing face challenges in producing 
		decoupled, local edits to 3D shapes. We address this problem 
		by learning disentangled latent representations that ground 
		language in 3D geometry. To this end, we propose a complementary 
		tool set including a novel network architecture, a disentanglement 
		loss, and a new editing procedure. Additionally, to measure edit 
		locality, we define a new metric that we call part-wise edit 
		precision. We show that our method outperforms existing SOTA 
		methods by 20% in terms of edit locality, and up to 6.6% in 
		terms of language reference resolution accuracy. Our work 
		suggests that by solely disentangling language representations, 
		downstream 3D shape editing can become more local to relevant 
		parts, even if the model was never given explicit part-based 
		supervision.  
	    </p>
        </div>
	</div>

   
    <div class="researchpaper">
	    <div class="paper-image">
		    <img src="assets/pictures/paper6.png" class="paper_pic">
	    </div>
	    <div class="paper-description">
		<h3 class=tab4> Language-Assisted 3D Shape Edits and Deformations</h3>
		<i> Submitted to ECCV2022. </i>
		<p>
	    In this work, we address the task of Language-Assisted 3D Shape Edits 
        and Deformations (which we name ChangeIt3D). Given a 3D representation 
        of an object and free-form natural language describing
        desired changes or modifications to the shape of the object, the task is
        to transform the input object’s geometry in a manner that reflects the
        requested changes – for example, to modify a 3D chair model to make
        its legs thinner, or to open a hole in its back. To tackle this problem in
        a way that promotes open-ended language usage allowing fine-grained
        shape edits, we introduce the largest existing corpus of natural language
        describing shape differences, which we call ShapeTalk. This dataset
        contains over half a million discriminative utterances produced by contrasting 
        the shapes of pairs of common 3D objects for a variety of object
        classes and degrees of similarity. We introduce metrics for the quantitative 
        evaluation of language-assisted shape editing methods that reflect
        key desiderata within this editing setup. We also design an effective and
        modular framework for ChangeIt3D that can combine an arbitrary 3D
        generative model of shapes with our in-house, ShapeTalk-based, text-to-shape 
        neural listener. Crucially, our modules are trained and deployed
        directly in a latent space of 3D shapes, bypassing the ambiguities of “lifting” 
        2D to 3D when using extant foundation models and thus opening a
        new avenue for 3D object-centric manipulation through language.	
		</p>
        </div>
	</div>



	<div class="researchpaper">
	    <div class="paper-image">
		    <img src="assets/pictures/paper5.png" class="paper_pic">
	    </div>
	    <div class="paper-description">
		<h3 class=tab4> PartGlot: Learning Shape Part Segmentation from Language Reference Games</h3>
		<i> Accepted to CVPR2022 (Oral). </i>
		<p>
		How did we as humans come to understand and identify the parts of objects in the world around us when we have never been explicitly given thousands of groundtruth segmentations? One answer, perhaps, lies in the way we can use language to refer and differentiate between different objects. In this paper, we hope to imbue neural networks with a similar capability: namely, by training the model to correctly identify an object from a small group of objects satisfying a natural language reference, we show that surprisingly nice part segmentation masks can be extracted out of the model's attention over the spatial domain <i> even when it was never given explicit part supervision.</i>
		</p>
		<p> Juil Koo, Ian Huang, Panos Achlioptas, Leonidas J. Guibas, Minhyuk Sung; Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2022, pp. 16505-16514 </p>
		<a href="assets/papers/part_glot.pdf" class=tab1>PDF</a>
        <a href="https://arxiv.org/abs/2112.06390">Arxiv</a>
        <a href="https://mhsung.github.io/papers/partglot.html">Project page</a>
        </div>
            
	</div>
	
        <div class="researchpaper">
            <div class="paper-image">
                <img src="assets/pictures/paper4.png" class="paper_pic">
            </div>
            <div class="paper-description">
                <h3 class=tab4> Functionality-driven Object Discovery in Videos</h3> 
                <p>AI needs to be able to reason about unfamiliar environments, objects, and entities. 
                I designed and developed a model that deduces the taxonomic relationship between seen and 
                unseen objects by observing human interactions with those classes in video data. I introduced 
                object-functionality projection, a novel technique to embed the functionality of objects as a 
                representation of their interactions with others in a latent space. It demonstrated significant 
                improvements upon baselines and prior works (at least a 27% relative increase – or 6% absolute – 
                in cumulative accuracy, and under some conditions, a 4x increase in accuracy).</p>
                <a href="assets/papers/2020CVPR_functionality_driven_object_discovery_in_videos.pdf" class=tab1>PDF</a>
            </div> 
        </div>
        

        <div class="researchpaper">
            <div class="paper-image">
                <img src="assets/pictures/paper3.png" class="paper_pic">
            </div>
            <div class="paper-description">
                <h3 class=tab4> Combating Catastrophic Forgetting without the Model </h3>
                <p>Catastrophic forgetting is the biggest bottleneck in online deep learning, and is critical to 
                lifelong learning in robots. I initiated a novel idea of using adversarial examples, a flaw of 
                neural networks, to fix catastrophic forgetting, another flaw of neural networks. I introduced 
                a new way of combating catastrophic forgetting in an online learning setting by optimizing input 
                images in an adversarial way without requiring the need for a specific type of architecture or 
                training loss function. We demonstrated that adversarial perturbations to the data is 
                capable of combating catastrophic forgetting.</p>
                <a href="assets/papers/2020AISTATS_combating_catastrophic_forgetting_without_the_model.pdf" class=tab1>PDF</a>
            </div> 
        </div>
       
        <div class="researchpaper">
            <div class="paper-image">
                <img src="assets/pictures/paper2.png" class="paper_pic">
            </div>
            <div class="paper-description">
                <h3 class=tab4> DeepBase: Deep Inspection of Neural Networks</h3>
                <p> We made Deep Neural Inspection (see below) faster and more scalable. We model logic
                with user-provided hypothesis functions that annotate the data with high-level
                labels like POS tags and image captions. Our system then provides sets of 
                neuronal units that show strong statistical correlation with the desired hypothesis.
                The optimizations in this project speed up standard Python implementation by 72X.</p>
                <p> T. Sellam, K. Lin, I. Huang, Y. Chen, M. Yang, C. Vondrick, 
                E. Wu, DeepBase: Deep Inspection of Neural Networks. SIGMOD 2019. </p>
                <a href="assets/papers/2019SIGMOD_DeepBase.pdf" class=tab1>PDF</a>
            </div>
        </div>
 
        <div class="researchpaper">
            <div class="paper-image">
                <img src="assets/pictures/paper1.png" class="paper_pic">
            </div>
            <div class="paper-description">
                <h3 class=tab4> Deep Neural Inspection </h3>
                <p> The explainability of deep learning systems remains a bottleneck. I worked on a system 
                to verify the internal logic of recurrent neural networks. I designed various experiments 
                that sought to validate the identification of important neuron groups. We introduced a 
                novel method called Deep Neural Inspection (DNI) to make sense of internal neuron-level 
                behaviors within recurrent neural networks to dissect and validate their logic for their decisions.</p>
                <p>T. Sellam, K. Lin, I. Huang, E. Wu, C. Vondrick. “I Like the Way You Think!” - Inspecting the Internal Logic of
                Recurrent Neural Networks. SysML Conference 2018.</p>
                <a href="assets/papers/2018SYSML_inspecting_internal_logic_of_RNN.pdf" class=tab1>PDF</a>
            </div> 
        </div>
        
                        
        </div>
 
    </div>
    <script src="js/common.js"></script>
    <script src="js/script_publications.js"></script>

</body>

</html>


